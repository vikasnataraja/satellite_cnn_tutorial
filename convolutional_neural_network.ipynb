{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe6d014-24b6-4b59-b750-88cf8bc8ac29",
   "metadata": {},
   "source": [
    "# Scene Classification of Satellite Imagery using a Convolutional Neural Network (CNN) - A Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c976e-177c-4514-bd94-351eb0eb3d70",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this tutorial is to show you how a convolutional neural network (CNN) works by walking you through the steps involved in building, train, and deploy one. In this example, we will try to train a CNN so that it learns how to categorize a given image into one of 'Forest', 'River', 'Highway', 'AnnualCrop', 'SeaLake', 'HerbaceousVegetation', 'Industrial', 'Residential', 'PermanentCrop', 'Pasture'. In machine learning terminology, these are often called `classes`. So, given a satellite image, the CNN should output one of these categories or _classes_. This type of framework is called **_scene classification_** and in remote sensing, this is an important task often employed for agriculture monitoring, disaster management, land cover change, etc.\n",
    "\n",
    "Let us start by importing some packages and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388592c1-b94b-4c6a-8b59-ab6eeb653c43",
   "metadata": {},
   "source": [
    "## Import necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd644816-cdde-46ad-acb1-70b073af02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future is imported to allow the use of different versions of Python\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # ignore pesky warnings for this tutorial :)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import xarray as xr\n",
    "\n",
    "# Keras and tensorflow are libraries for deep learning which we will use to build the neural network\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4434c58-b431-4b40-b2a0-34487c256f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.getcwd() # current directory, feel free to change it to whatever desired location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431980b-29f1-4132-8a96-ecaa57858715",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The data set comes courtesy of [EuroSAT](https://github.com/phelber/eurosat), put together using data from the [European Copernicus Sentinel-2 mission](https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-2) that utilizes wide-swath, high-resolution, multi-spectral imaging _\"to support operational applications primarily for land services, including the monitoring of vegetation, soil and water cover, as well as the observation of inland waterways and coastal areas\"_. \n",
    "\n",
    "![Sentinel satellite](assets/sentinel_satellite_with_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3ea3e-1fff-4a8b-b02b-f25046bafb8a",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "\n",
    "Before we start building a machine learning model, we must first always understand what the data looks like and what inherent properties we must be aware of. This, in turn, will help to determine the type of model and its structure. \n",
    "\n",
    "This particular data set we will see for this tutorial covers 13 spectral bands (we will only use the RGB channels) with 10 labeled classes on 27,000 geo-referenced images. Each image is 64 x 64 x 3 pixels (flattened). Let's load the netCDF4 file using `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1be165-1fdb-4e94-a363-11bf61ebe7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_ds = xr.open_dataset('data/eurosat.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e7d1c-f867-4f25-9952-da0664a9c124",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Data visualization should almost always be the first step in any machine learning project. This is also sometimes referred to as **\"Exploratory Data Analysis\" or EDA**. Playing around with your data and visualizing it as images, histograms, plots, etc., will allow you to get a sense of what kinds of properties your data has (is it imbalanced? is it biased? is it corrupted? etc.). Once you know that, you can start cleaning up the data, doing any processing if needed before you move on to the algorithm stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5707a35-b37e-42e4-8310-860d077d1b8f",
   "metadata": {},
   "source": [
    "### Imagery\n",
    "\n",
    "Here, let's create a figure of our imagery in a 4 x 5 grid and sample randomly from our data set to display them as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824abd5-17c9-4a85-8b74-05bbfa6bae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "columns = 4\n",
    "rows = 5\n",
    "random_idx = np.random.randint(0, sat_ds.dims['n_imgs'] - 1, size=rows * columns) # random indices to select from sat_ds\n",
    "\n",
    "for i in range(0, columns*rows):\n",
    "    \n",
    "    sample_ds = sat_ds.isel(n_imgs=random_idx[i])\n",
    "    img = sample_ds['pixel_data'].values\n",
    "    img = img.reshape((64, 64, 3)).astype('int') # because we had flattened it for xarray\n",
    "    label = sample_ds['label'].values\n",
    "    \n",
    "    ax = fig.add_subplot(rows, columns, i + 1)\n",
    "    ax.imshow(img, origin='lower')\n",
    "    ax.set_title('Label = {}'.format(label))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e6dae-2f95-47b9-ae1a-77e68879f214",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Does our data set have the same number of images for every \"class\" or \"label\" or \"scene\"? Or is it imbalanced? Let's use a bar plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69588cd-447c-43da-be63-9c9b3d6f532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique labels are there\n",
    "labels, counts = np.unique(sat_ds['label'].values, return_counts=True)\n",
    "n_labels = labels.size\n",
    "\n",
    "# create some nice colors from matplotlib tableau colors\n",
    "bar_colors = list(matplotlib.colors.TABLEAU_COLORS.values())\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.grid(axis='y', alpha=0.5) # set grid for just the y-axis with a 0.5 transparency\n",
    "ax.bar(labels, counts, color=bar_colors, zorder=3) # plot it!\n",
    "ax.set_ylabel('Number of samples available')\n",
    "# no need to set x label as it should be self-evident\n",
    "ax.set_title('Distribution of classes in the EuroSAT data set', fontweight='bold', fontsize=18) # make it bigger and bolder\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2b018-5074-4963-b209-70ade39f07f7",
   "metadata": {},
   "source": [
    "We can see that most of the data set is more or less equally distributed across different classes. The `pasture` class has the lowest number of images to its name, all others have between 2,500 to 3,000 images.\n",
    "\n",
    "_**Future work**_: Make a more comprehensive set of analyses on this data set. Here, we only went through a couple of analyses but you will want to be more thorough and detailed, especially if the data set is more complex than this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0ce06-adf0-4cb7-b180-613fdff19ec9",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Now that we have a visual and statistical sense of what our data set looks like, let's move on to other tasks. **Remember that we want our machine learning model (in this case, a CNN) to be able to look at one of these images and predict the _\"label\"_ or _\"class\"_.**\n",
    "\n",
    "You may ask yourself - why is this important? Why do we want to build a machine learning model to tell us if the satellite is seeing a forest or a river if we already know what the answer? Doesn't this data set already have the labels?\n",
    "\n",
    "Well, consider this - the data set here was manually labeled (which is often the case in building machine learning data sets) where researchers arduously went through tens of thousands of images and labeled them. It takes time and effort (not to mention the financial cost!) to build these huge data sets that ML models often require. And when you take into account that <ins>satellites like Sentintel-2 collect over 2.5 Tb of data every single day</ins>, we cannot possibly label every single image! So, if our ML model can learn on this data set, we can then apply it to new imagery to tell us what the satellite is seeing without having to spend years manually labeling our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac7c74-1652-4357-bfb6-b5f18c727273",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "### 1. Convert `labels` into numbered `classes`\n",
    "\n",
    "We currrently have our `sat_ds` Xarray Dataset with pixel values in `pixel_data` and the labels (also called ground truth in ML) stored as strings like 'Forest', 'River', etc. But the model will expect a digital value, so to speak. So, let's map each of these labels to an integer value. The order doesn't matter since we will shuffle them anyway. These integer values are the _\"answers\"_ to each image that the model will predict eventually. They are called `classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f65a96-0838-4924-aeb3-b0b3992ff1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary mapping names to integers (or \"classes\")\n",
    "label_to_class_map = {'Forest': 0,\n",
    "                     'River': 1,\n",
    "                     'Highway': 2,\n",
    "                     'AnnualCrop': 3,\n",
    "                     'SeaLake': 4,\n",
    "                     'HerbaceousVegetation': 5,\n",
    "                     'Industrial': 6,\n",
    "                     'Residential': 7,\n",
    "                     'PermanentCrop': 8,\n",
    "                     'Pasture': 9\n",
    "                    }\n",
    "\n",
    "class_to_label_map ={0: 'Forest',\n",
    "                     1: 'River',\n",
    "                     2: 'Highway',\n",
    "                     3: 'AnnualCrop',\n",
    "                     4: 'SeaLake',\n",
    "                     5: 'HerbaceousVegetation',\n",
    "                     6: 'Industrial',\n",
    "                     7: 'Residential',\n",
    "                     8: 'PermanentCrop',\n",
    "                     9: 'Pasture'}\n",
    "\n",
    "\n",
    "def convert_scene_label_to_class_xarray(xr_ds):\n",
    "    \"\"\" \n",
    "    Convert labels like Forest, River, etc. to integer labels (0, 1, ...) as classes\n",
    "    and add it to the xarray dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    string_labels = xr_ds['label'].values\n",
    "    \n",
    "    # use np.vectorize to apply the mapping to the array\n",
    "    int_array = np.vectorize(label_to_class_map.get)(string_labels)\n",
    "\n",
    "    # add it to the xarray dataset\n",
    "    xr_ds['class'] = xr.DataArray(int_array, dims=('n_imgs', ), coords=dict(n_imgs=('n_imgs', xr_ds.coords['n_imgs'].values)))\n",
    "\n",
    "    return xr_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aee79f-354f-40aa-82f4-9c28efb8799d",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "### 2. Split the data set\n",
    "\n",
    "We need to split our data set into three different sets:\n",
    "\n",
    "* training data set - this is the \"labeled\" data that the model will use to learn. it will look at each image in the training set, forward calculate the prediction, and then check that prediction against the given label using a loss function (also called error function or ~ cost function).\n",
    "* validation data set - this is also \"labeled\" data but used for evaluating how well the model is learning. So, the model won't learn from the validation data, but by evaluating on it, you (the user) can get a sense of how well your model is built and start to debug if it isn't.\n",
    "* testing data set - this is the data set which the model has never seen before and therefore provides a true measure of how well the model has been trained.\n",
    "\n",
    "So, we need to split our large data set into these 3 components. Generally speaking, we want the training data set to be the largest, the testing data set to be the second largest, followed by the validation. But in ML, this is highly dependent on the actual data and the application and so on. For our use case on the EuroSAT data set, let's use a commonly followed split ratio of 80:10:10 meaning 80% of the data is for training, and 10% each for validation and testing. And let us also shuffle our data to eliminate any ordered biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ce5dd-a6bd-411a-851f-441f3f97eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(xr_ds, val_fraction=0.1, test_fraction=0.1, normalize=True):\n",
    "    \"\"\"Splits the xarray dataset into training, validation, and testing sets.\n",
    "    Note: Assumes that X = 'pixel_data', y = 'label'\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        xr_ds: xarray dataset\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test (ndarray): Split data into train, test and val (X and y).\n",
    "    \"\"\"\n",
    "\n",
    "    # first, shuffle the data by simply creating indices and shuffling them \n",
    "    # let us also set a random seed to reproduce the shuffle every time\n",
    "    np.random.seed(73) # in place\n",
    "\n",
    "    n_imgs = xr_ds.sizes['n_imgs']\n",
    "    ds_idx = np.arange(0, n_imgs, 1) # data set indices\n",
    "    np.random.shuffle(ds_idx) # shuffle indices in-place\n",
    "\n",
    "    X = xr_ds['pixel_data'].values\n",
    "    y = xr_ds['class'].values\n",
    "\n",
    "    # calculate the split index \n",
    "    split_index = int(n_imgs * (1 - test_fraction - val_fraction))\n",
    "    n_train = split_index\n",
    "    n_val   = int(n_imgs * val_fraction)\n",
    "    n_test  = int(n_imgs * test_fraction)\n",
    "\n",
    "    # split the data \n",
    "    X_train, y_train = X[ds_idx[0:split_index]], y[ds_idx[0:split_index]]\n",
    "\n",
    "    X_val, y_val     = X[ds_idx[split_index:split_index+int(val_fraction * n_imgs)]], y[ds_idx[split_index:split_index+int(val_fraction * n_imgs)]]\n",
    "\n",
    "    X_test, y_test   = X[ds_idx[split_index+int(val_fraction * n_imgs):split_index+int(val_fraction * n_imgs)+int(test_fraction * n_imgs)]], y[ds_idx[split_index+int(val_fraction * n_imgs):split_index+int(val_fraction * n_imgs)+int(test_fraction * n_imgs)]]\n",
    "\n",
    "    # since our pixel data in the xarray dataset is all 1D, we need to shape them into 3D arrays of n_imgs x 64 x 64 x 3\n",
    "    train_shape, val_shape, test_shape = (n_train, 64, 64, 3), (n_val, 64, 64, 3), (n_test, 64, 64, 3)\n",
    "    X_train, X_val, X_test = X_train.reshape(train_shape), X_val.reshape(val_shape), X_test.reshape(test_shape)\n",
    "\n",
    "    if normalize: # normalize by 255 i.e., the max value\n",
    "        X_train, X_val, X_test = X_train/255, X_val/255, X_test/255\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e30f6-98e8-4eae-91d2-6994ef0cda2e",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "### 3. Make our `classes` probabilistic\n",
    "\n",
    "This is the final data processing step. Since we have 10 different types of scenes, let us have our model generate a PDF spread across the classes. To do so, we must one-hot encode the `classes` which means to convert the 10 classes each to binary maps. For instance, if for a given image, the ground truth class is `4` (which is a `SeaLake` label), we need to assign a probability of 1 to that class, and a 0 to all the other classes. So, our `classes` which is of shape (`n_imgs`) will instead become (`n_imgs`, 10) meaning each image will have 10 probabilities whose sum will be 1.\n",
    "\n",
    "Fortunately, `keras` offers a funtion `to_categorical` to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679124a-a043-439f-84bb-d28ac36c63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_classes(classes, num_classes):\n",
    "    \"\"\"\n",
    "    One-hot encode classes into num_classes of binary probabilities\n",
    "    \"\"\"\n",
    "    return keras.utils.to_categorical(classes, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a03a0c-83d2-4b46-8bc3-992141c24db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together\n",
    "validation_data_fraction = 0.1 # 10% of the data\n",
    "test_data_fraction       = 0.1 # 10% of the data\n",
    "num_classes = 10\n",
    "normalize   = True # whether to normalize the pixels between 0 and 1\n",
    "\n",
    "\n",
    "# Step 1: convert labels to classes\n",
    "sat_ds = convert_scene_label_to_class_xarray(sat_ds)\n",
    "\n",
    "# Step 2: split the data set\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(sat_ds, \n",
    "                                                                      val_fraction=validation_data_fraction, \n",
    "                                                                      test_fraction=test_data_fraction,\n",
    "                                                                      normalize=normalize)\n",
    "\n",
    "# Step 3: make the y classes probabilistic using one-hot encoding\n",
    "y_train = one_hot_encode_classes(y_train, num_classes)\n",
    "y_val   = one_hot_encode_classes(y_val, num_classes)\n",
    "y_test  = one_hot_encode_classes(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc0183f9-bf7a-4b23-ab28-f1862cd905c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443d541-6400-4ec9-9225-62c4762c63f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a094846-c226-4854-87bc-556c27a60e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a directory where the model will be saved\n",
    "# if it doesn't exist, ensure that it will be created\n",
    "\n",
    "model_dir = os.path.join(parent_dir, 'saved_models')\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445876f-da8c-41c1-af26-c6fb4cfeaac8",
   "metadata": {},
   "source": [
    "## Model - Convolutional Neural Network\n",
    "\n",
    "The exciting part! Let's build a convolutional neural network (CNN)! Remember:\n",
    "\n",
    "**Input**  = RGB images\n",
    "\n",
    "**Output** = A set of probabilities (PDF) spread across 10 classes \n",
    "\n",
    "\n",
    "![CNN Architecture](assets/cnn_architecture.png)\n",
    "\n",
    "### Convolution (Conv2D)\n",
    "\n",
    "This layer creates a convolution kernel that is _convolved_ with whatever input is given to it from the previous stage over a 2D spatial (or temporal) dimension (height and width) to produce an output. It comes with an activation function which essentially applies a non-linear function to the neuron output. You could also make it linear but it is recommended to use non-linear activation functions like `tanh` (hyperbolic tangent) or `relu` (REctified Linear Unit).\n",
    "\n",
    "### Max Pooling (MaxPooling2D)\n",
    "\n",
    "Downsamples the input along its spatial dimensions (height and width) by only taking the maximum value over an input window (of size defined by `pool_size`) for each channel of the input. Downsampling helps CNNs learn the signal in the data better by filtering out the noise. \n",
    "\n",
    "### Dense (Fully connected layer)\n",
    "\n",
    "The Dense layer (also called fully connected layer) implements the operation: `output = activation(dot(input, kernel) + bias)`. They connect every single input to the output and force the model to learn from every single one of them. The non-linear activation function then decides which ones are useful and which ones aren't. Dense layers capture and learn from the abstract feature representation by using a compressed version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68551f-a999-4395-a748-0b858e3de753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the convolutional neural network that looks something like this:\n",
    "# input -> convolution -> downsample -> convolution -> downsample -> convolution -> downsample -> dense -> dense\n",
    "\n",
    "def model_architecture(input_data_shape, num_filters, kernel_size, num_dense_units, num_classes):\n",
    "    # create a sequential model where the layers can be added easily.\n",
    "    # for this example, we will use 3 convolutional layers, 2 pooling (downsampling) layers,\n",
    "    # and 2 dense (fully connected) layers.\n",
    "    # feel free to add or remove some layers to see what happens!\n",
    "    \n",
    "    model = keras.Sequential(\n",
    "    [\n",
    "        Input(shape=input_data_shape),\n",
    "        Conv2D(num_filters[0], kernel_size=kernel_size, activation=\"relu\"), # convolve\n",
    "        MaxPooling2D(pool_size=(2, 2)), # halve the number of spatial dimensions\n",
    "        Conv2D(num_filters[1], kernel_size=kernel_size, activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(num_filters[2], kernel_size=kernel_size, activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)), \n",
    "        Flatten(), # flatten due to keras backend requirements, not a learnable layer\n",
    "        Dense(num_dense_units[0], activation=\"relu\"), # the fully connected layer is called Dense in keras, \n",
    "        Dense(num_dense_units[1], activation=\"relu\"), \n",
    "        Dense(num_classes, activation=\"softmax\"), # we want probability as the output with num_classes\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8c964-0204-418f-bab2-bf5c7f43c175",
   "metadata": {},
   "source": [
    "_**Future Work**_: Play around with the model by adding/removing layers. Change the kernel size, number of filters, padding, etc., to see how it influences the training and prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf7c13-0c00-43db-b05e-0d3187e69de7",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "These are the \"knobs\" you can turn and change to your liking to optimize the training process. \n",
    "\n",
    "## Batch size\n",
    "\n",
    "The number of images to be taken and trained before updating the weights. This is done to reduce the memory consumption. Instead of training all images at the same time, we do it batch-by-batch.\n",
    "\n",
    "- Batch sizes are usually in powers of 2 e.g., 8, 16, 32, 64, 128 ...\n",
    "- For example if your total number of training images = 2000 and batch size is 128 then we get 15 full batches and the final batch will have the remaining images.\n",
    "- Higher batch size almost always gives better accuracies but will be computationally slow and take up more memory on your computer which can crash\n",
    "\n",
    "## Learning rate \n",
    "\n",
    "Remember that CNNs learn by changing or updating the weights through backpropagating the error. So, if our weights are $w$, and the error is calculated as some function of our weights, $E(w)$, then using the error, the weights in the next time step are updated as:\n",
    "\n",
    "$w_{t+1} = w_{t} - \\alpha \\dfrac{\\partial{E}}{\\partial{w}}$\n",
    "\n",
    "The $\\alpha$ is the learning rate. It is a number between 0 and 1 and dictates how fast your optimization moves. A popular optimization algorithm is gradient descent. Learning rates are usually set in tenths like 0.1, 0.01 etc,. It is a good idea to start with a low value and gradually increase or decrease.\n",
    "\n",
    "## Number of epochs\n",
    "\n",
    "1 epoch = the model has seen one full pass of the entire training set.\n",
    "Since we use batches, 1 epoch will be competed after (dataset_size/batch_size) steps\n",
    "Generally speaking, it is common to train for 100,000 or even 500,000 epochs. For this example we choose a small number.\n",
    "\n",
    "## Tuning the hyperparameters\n",
    "Feel free to play around with some of these \"hyperparameters\". For instance, you could modify the batch_size to be 8 instead of 16.\n",
    "\n",
    "You could also run the entire model as is first and then reset, come back here and change something, run the model again to see what changes. For example,\n",
    "\n",
    "* If the model is too slow to train, try decreasing the `batch_size`.\n",
    "* If the loss is still high after training, try increasing the number of `epochs`\n",
    "* If the model is learning too slowly or is not converging faster, try increasing the `learning_rate` (small steps)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663851c7-7fd7-47db-b210-164b7b991f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define architecture details\n",
    "\n",
    "input_shape = (64, 64, 3)    # because our images are 64 x 64 x 3 pixels across\n",
    "\n",
    "# hyperparameters\n",
    "num_filters = [16, 32, 64]   # filters for convolution it's usually a good idea to double the filters but feel free to play around with this\n",
    "kernel_size = (3, 3)         # this is the convolution kernel\n",
    "num_epochs  = 15             # number of epochs to train for\n",
    "learning_rate = 0.001        # learning rate for the model weights. recommended to start with a low number like 0.0001\n",
    "batch_size = 16              # number of images that the network will see at once. higher batch size = more memory required \n",
    "num_dense_units = [400, 200] # number of dense units for the dense layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcaa65-5c66-405f-bfde-83039b828e5e",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The loss function (sometimes referred to as error or cost function) is one of the most important parts of a machine learning model. It dictates the metric by which your model will learn. For instance, if you are simply looking for the model to output a number (regression), you will want to use something like mean squared error (MSE). In our case, since we want to output probabilities, we need something different. That something could be a loss function called categorical cross entropy which looks like this:\n",
    "\n",
    "If the probability that a given image belongs to a class $x$ is $p(x)$ and the CNN outputs a probability density function denoted as $\\hat{p}(x)$, then we can write our cross entropy loss function simply as:\n",
    "\n",
    "$Loss_{CE} = -\\sum_{x\\in classes} p(x) * log (\\hat{p}(x))$\n",
    "\n",
    "We will use the `model.compile` function in Keras to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c56f4c-946f-412d-80f6-05ee265d7127",
   "metadata": {},
   "source": [
    "## Train the CNN!\n",
    "\n",
    "Right then, we've gone through all the data processing and model building stages. Let's put it all together and `train` our model. In other words, let's have the model learn on the (`X_train`, `y_train`) data set by _\"fitting\"_ the model to the data. This is the `model.fit` function built in to Keras.\n",
    "\n",
    "We will also save the model using timestamps so that you can re-run this cell to re-train the model without having to rename it every time. And while we're at it, let's stop the model if it doesn't learn or improve for more than 15 epochs because that likely means that the model is stuck in a local minimum but thinks it is at the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51653638-f4e8-4d04-8e09-57b82b575ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the model with a date and time so you can run new models without having to change this name every time\n",
    "model_name  = 'my_eurosat_cnn_{}.h5'.format(datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S'))    # cnn will be saved under this filename\n",
    "\n",
    "# build the model\n",
    "model = model_architecture(input_shape, num_filters, kernel_size, num_dense_units, num_classes)\n",
    "\n",
    "# callbacks to the model\n",
    "# 1. stop the model early if it is not learning anything\n",
    "# 2. save the model (checkpoint) only if the loss has improved i.e., if the model was able to learn something new or better\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=15, verbose=1),\n",
    "             keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_dir, model_name), monitor=\"val_loss\", save_best_only=True, verbose=1)]\n",
    "\n",
    "print('Model will be saved to {}\\n'.format(os.path.join(model_dir, model_name)))\n",
    "\n",
    "# our cost function or loss function is cross-entropy which is probabalistic\n",
    "# use the adaptive momentum optimizer algorithm\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # fit the model to the dataset X: train_images, y: train_labels\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f4d20-8ea9-4640-9b0e-482bde50607b",
   "metadata": {},
   "source": [
    "## Analyze how the training went\n",
    "\n",
    "The training and validation loss should be close to each other and decrease over time (epochs).\n",
    "\n",
    "* If training loss is low, but validation loss is much higher, that means the model _\"overfitted\"_\n",
    "    * That means the model probably memorized the training data and when confronted with a new, unseen image, did not know how to respond\n",
    "\n",
    "* If both losses are high, that means the model did not learn anything and _\"underfitted\"_\n",
    "\n",
    "* If training loss is low, and validation loss is also low, that means the model did a good job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236ba8b-2d93-42e8-bb3b-6cf3e3daa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# first plot will be of accuracies\n",
    "ax[0].plot(history.history['accuracy'], color='teal', label='training')\n",
    "ax[0].plot(history.history['val_accuracy'], color='orange', label='validation')\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# second plot will be of the errors\n",
    "ax[1].plot(history.history['loss'], color='teal', label='training')\n",
    "ax[1].plot(history.history['val_loss'], color='orange', label='validation')\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(['training', 'validation'])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282f8df-fdc0-4eb1-9584-fefd8bbdea80",
   "metadata": {},
   "source": [
    "## Test the trained model on test set\n",
    "\n",
    "Now that the model has been trained, you can test it on new images i.e your test images. This is really what matters because it is a true test of how well the model can generalize to a never before seen set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1868d-5074-470b-b77a-bef34d95afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score = model.evaluate(X_val, y_val, verbose=0)\n",
    "test_score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Validation data set loss: {:0.2f}\".format(val_score[0]))\n",
    "print(\"Validation data set accuracy: {:0.2f}\".format(val_score[1]))\n",
    "\n",
    "print(\"\\nTesting data set loss: {:0.2f}\".format(test_score[0]))\n",
    "print(\"Testing data set accuracy: {:0.2f}\".format(test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aff4b-fe45-42c2-9ff3-377a871a406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to predict on the testing data set\n",
    "predictions = model.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883bfcc-cc4a-42f6-a5e5-70c2e86c3e62",
   "metadata": {},
   "source": [
    "## Visualize the results\n",
    "\n",
    "Let's also make it so that we draw randomly from our testing data set so that you can run the cell below repeatedly to produce different images and see how the model predicted on each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55706b-a270-4710-b9f2-7605d1b62b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each time this cell is run, a different set of images will be displayed randomly\n",
    "\n",
    "# display the results\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "columns = 4\n",
    "rows = 4\n",
    "\n",
    "random_samples = np.random.randint(0, len(X_test), size=rows * columns) # random indices to select from X_test\n",
    "\n",
    "for i in range(0, columns*rows):\n",
    "    ax = fig.add_subplot(rows, columns, i + 1)\n",
    "\n",
    "    ax.imshow(X_test[random_samples[i]], origin='lower')\n",
    "    predicted_label = class_to_label_map[np.argmax(predictions[random_samples[i]])]\n",
    "    actual_label    = class_to_label_map[np.argmax(y_test[random_samples[i]])]\n",
    "    confidence      = np.max(predictions[random_samples[i]]) # just the maximum probability\n",
    "    ax.set_title('Predicted label = {}\\nConfidence = {:0.2f}\\nActual label = {}'.format(predicted_label, confidence, actual_label), pad=5, fontsize=12)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9741f-8dfa-42e9-9eda-d70c181fa57d",
   "metadata": {},
   "source": [
    "## Test it on random satellite images\n",
    "\n",
    "Let's see if the model can identify what the scene is if it is given a random satellite image. There are a few such images in the `random_images/` directory. Let's read and make the model predict on them. The ground truth (defined vaguely here on purpose) is in the filename.\n",
    "\n",
    "Note that this is an exercise to test the limits of the model and what data it was trained on. Not all satellite images are the same. Another consideration is the training data itself - the EuroSAT data had specific labeled scenes so anything outside of those regimes will likely confuse the model. Of course, maybe the model is so used to seeing data like that in the EuroSAT that it just memorized it and doesn't know what any other satellite images look like. Still, let's see if this breaks the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a6141-26ba-4677-9ed5-b61bdda53979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames and therefore ground truths\n",
    "rand_img_dir = 'random_images/'\n",
    "random_imgs  = [f for f in os.listdir(rand_img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# create a 4 x 1 grid of images\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "columns = 4\n",
    "rows = 1\n",
    "\n",
    "for i in range(0, columns*rows):\n",
    "    # first read in the image\n",
    "    img = Image.open(os.path.join(rand_img_dir, random_imgs[i]))\n",
    "\n",
    "    # if the image is not 64 x 64, then attempt to resize it\n",
    "    if img.size != (64, 64):\n",
    "        try:\n",
    "            img = img.resize((64, 64))\n",
    "        except Exception as err:\n",
    "            print('Encountered the following error while resizing {}: {}'.format(os.path.join(rand_img_dir, random_imgs[i]), err))\n",
    "            continue # skip this image then\n",
    "\n",
    "    # for a single image, the model is expecting an input shape of (1, 64, 64, 3) so let's pad the first dimension\n",
    "    img = np.array(img)\n",
    "    img_for_model = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # and finally normalize it\n",
    "    img_for_model = img_for_model/255\n",
    "\n",
    "    # now let's use the model to predict on it\n",
    "    img_prediction = model.predict(img_for_model, verbose=0)\n",
    "\n",
    "    # same as before, convert numerical value to label and get the confidence i.e., max probability\n",
    "    predicted_label = class_to_label_map[np.argmax(img_prediction)]\n",
    "    actual_label    = random_imgs[i].split('.')[0] # just the filename itself without the extension\n",
    "    confidence      = np.max(img_prediction)\n",
    "    \n",
    "    ax = fig.add_subplot(rows, columns, i + 1)\n",
    "    ax.imshow(img, origin='lower')\n",
    "    ax.set_title('Predicted label = {}\\nConfidence = {:0.2f}\\nActual label = {}'.format(predicted_label, confidence, actual_label), pad=5, fontsize=12)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0fc6d-48aa-4520-8ac9-3847c6b333c4",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that you've trained the first iteration of your model, how about we go check what happens when:\n",
    "* you change the hyperparameters. Remember to change one thing at a time so you can keep track of which ones affect your model the most. For instance:\n",
    "    * increase the `test_data_fraction` to 0.2 instead of 0.1\n",
    "    * decrease the `batch_size` to 8\n",
    "    * increase the `num_epochs` to 40\n",
    "* add another or remove a convolutional layer to/from your model\n",
    "* add another dense layer to your model\n",
    "* normalize/un-normalize the input RGB images\n",
    "* change the optimizer to \"sgd\" instead of \"adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fdf91-71ab-4f95-ad4d-860b6159c502",
   "metadata": {},
   "source": [
    "# Go Further\n",
    "\n",
    "Here are some more examples to try out:\n",
    "\n",
    "* A CNN to classify handwritten digits (you can upload a hand-drawn picture of a number to try out!): [MNIST Classifier](https://github.com/vikasnataraja/Handwritten-Digit-Classification-using-TensorFlow/blob/master/convolutional_network-mnist.ipynb)\n",
    "* Several examples of deep learning in jupyter notebooks by Francois Challet, the creator of Keras: [Keras CNNs](https://github.com/fchollet/deep-learning-with-python-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f95488-6662-4056-8dd8-45ee8e7d73c1",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n",
    "\n",
    "Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018.\n",
    "\n",
    "Original Data Repo: https://github.com/phelber/eurosat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
